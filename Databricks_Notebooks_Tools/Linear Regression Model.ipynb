{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6de37019-6751-43a0-a2eb-e9d557bcaafc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import (\n",
    "    StringIndexer, OneHotEncoder, VectorAssembler, Imputer, StandardScaler\n",
    ")\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.ml.functions import vector_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09417885-aaaf-4903-aa84-62502f037bf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "\n",
    "# 1. File Paths & IDs\n",
    "INPUT_PATH = \"/mnt/lab94290/cluster_19/airbnb_data_updateddd\"  \n",
    "dbutils.widgets.text(\"property id\", \"40458495\")\n",
    "\n",
    "TARGET_PROPERTY_ID = dbutils.widgets.get(\"property id\").strip()                  \n",
    "# 2. Column Names\n",
    "ID_COL = \"property_id\"\n",
    "LABEL_COL = \"ratings\"\n",
    "AMENITIES_RAW_COL = \"amenities\"\n",
    "\n",
    "# 3. Currency Conversion (Table based)\n",
    "FX_RATES_DATA = [(\"USD\", 1.0), (\"EUR\", 1.05)] \n",
    "FX_RATES_DF = spark.createDataFrame(FX_RATES_DATA, [\"currency\", \"to_usd\"])\n",
    "\n",
    "# 4. Features Lists\n",
    "NUMERIC_FEATURES_BASE = [\n",
    "    \"price_per_guest\"   \n",
    "]\n",
    "\n",
    "BOOL_FEATURES_BASE = [\n",
    "    \"is_super_host\" # 0/1\n",
    "]\n",
    "\n",
    "CATEGORICAL_FEATURES = [\n",
    "    \"cancellation_bucket\"       \n",
    "]\n",
    "\n",
    "CATEGORY_RATING_COLS = [\n",
    "    \"cat_cleanliness\",          \n",
    "    \"cat_checkin\",\n",
    "    \"cat_communication\"\n",
    "]\n",
    "\n",
    "# 5. Global Amenities to always include\n",
    "GLOBAL_IMPORTANT_AMENITIES = [\n",
    "    \"Wifi\", \"Air conditioning\", \"Heating\", \"Washer\", \"Dryer\",\n",
    "    \"Free parking on premises\", \"Pool\", \"Hot tub\", \"Kitchen\", \"TV\", \"Hair dryer\"\n",
    "]\n",
    "\n",
    "# 6. Schema for Amenities parsing\n",
    "AMENITIES_SCHEMA = T.ArrayType(\n",
    "    T.StructType([\n",
    "        T.StructField(\"group_name\", T.StringType(), True),\n",
    "        T.StructField(\"items\", T.ArrayType(\n",
    "            T.StructType([\n",
    "                T.StructField(\"name\",  T.StringType(), True),\n",
    "                T.StructField(\"value\", T.StringType(), True)\n",
    "            ])\n",
    "        ), True)\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "feece70f-e9a9-4c50-9150-63ff06500fe9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_neighbors_by_geo_bucket(df, property_id, id_col=\"property_id\", res_col=\"geo_bucket_res\", bucket_col=\"geo_bucket\", h3_prefix=\"h3_\"):\n",
    "    target = df.filter(F.col(id_col) == F.lit(property_id)).select(res_col, bucket_col).limit(1).collect()\n",
    "    \n",
    "    if not target:\n",
    "        raise ValueError(f\"property_id {property_id} not found\")\n",
    "\n",
    "    res = int(target[0][res_col])\n",
    "    bucket = target[0][bucket_col]\n",
    "    h3_col = f\"{h3_prefix}{res}\"\n",
    "    \n",
    "    # Filter neighbors\n",
    "    neighbors_df = df.filter(F.col(h3_col) == F.lit(bucket))\n",
    "    return neighbors_df\n",
    "\n",
    "def extract_features(df, fx_rates_df):\n",
    "    # 1. Currency Conversion\n",
    "    df = df.withColumn(\"currency\", F.when(F.col(\"currency\").isNull(), F.lit(\"USD\")).otherwise(F.col(\"currency\")))\n",
    "    df = df.join(fx_rates_df, on=\"currency\", how=\"left\")\n",
    "    df = df.withColumn(\"price_usd\", F.col(\"price\") * F.col(\"to_usd\"))\n",
    "    \n",
    "    # 2. Price per Guest\n",
    "    df = df.withColumn(\"guests\", F.when(F.col(\"guests\").isNull() | (F.col(\"guests\") <= 0), F.lit(1.0)).otherwise(F.col(\"guests\")))\n",
    "    df = df.withColumn(\"price_per_guest\", F.when((F.col(\"price_usd\").isNotNull()), F.col(\"price_usd\") / F.col(\"guests\")).otherwise(None))\n",
    "\n",
    "    # 3. Cancellation Policy Parsing\n",
    "    df = df.withColumn(\"cp_text\", F.lower(F.col(\"cancellation_policy\").cast(\"string\")))\n",
    "    df = df.withColumn(\"cancellation_bucket\",\n",
    "        F.when(F.col(\"cp_text\").contains(\"no refund\"), \"strict\")\n",
    "        .when(F.col(\"cp_text\").contains(\"partial refund\"), \"moderate\")\n",
    "        .when(F.col(\"cp_text\").contains(\"full refund\"), \"flexible\")\n",
    "        .otherwise(\"unknown\")\n",
    "    )\n",
    "\n",
    "    # 4. Extract Category Ratings\n",
    "    def extract_cat(name):\n",
    "        return F.regexp_extract(F.col(\"category_rating\").cast(\"string\"), f'\"name\":\"{name}\".*?\"value\":\"([0-9\\\\.]+)\"', 1).cast(\"double\")\n",
    "\n",
    "    df = df.withColumn(\"cat_cleanliness\", extract_cat(\"Cleanliness\"))\n",
    "    df = df.withColumn(\"cat_checkin\", extract_cat(\"Check-in\"))\n",
    "    df = df.withColumn(\"cat_communication\", extract_cat(\"Communication\"))\n",
    "\n",
    "    # 5. Type Casting for Numerics (Ensuring Double Type for ML)\n",
    "    for c in NUMERIC_FEATURES_BASE + BOOL_FEATURES_BASE:\n",
    "        if c in df.columns:\n",
    "            df = df.withColumn(c, F.col(c).cast(\"double\"))\n",
    "            \n",
    "    # Target Label Casting\n",
    "    if LABEL_COL in df.columns:\n",
    "        df = df.withColumn(LABEL_COL, F.col(LABEL_COL).cast(\"double\"))\n",
    "            \n",
    "    return df\n",
    "\n",
    "def build_amenity_feature_df(df_in, id_col, top_k_local=15, global_list=None, raw_col=\"amenities\"):\n",
    "    \"\"\" Optimized version using manual aggregation instead of slow pivot \"\"\"\n",
    "    if global_list is None: global_list = []\n",
    "    \n",
    "    # 1. Parse JSON\n",
    "    df_parsed = df_in.withColumn(\"amenities_parsed\", F.from_json(F.col(raw_col).cast(\"string\"), AMENITIES_SCHEMA))\n",
    "    \n",
    "    # 2. Explode to find top amenities - FIXED (Double Explode for Group -> Items)\n",
    "    amenities_long = (\n",
    "        df_parsed.select(F.col(id_col).alias(\"pid\"), F.explode_outer(\"amenities_parsed\").alias(\"group\")) # קודם מפרקים קבוצות\n",
    "        .select(\"pid\", F.explode_outer(\"group.items\").alias(\"item\")) # ואז מפרקים את הפריטים בתוך הקבוצה\n",
    "        .select(\"pid\", F.lower(F.col(\"item.name\")).alias(\"name\")) # עכשיו השדה הוא String תקין\n",
    "        .filter(F.col(\"name\").isNotNull())\n",
    "    )\n",
    "    \n",
    "    # Simple normalization (tv variants, etc.)\n",
    "    amenities_long = amenities_long.withColumn(\"name\", \n",
    "        F.when(F.col(\"name\").contains(\"tv\"), \"tv\")\n",
    "         .when(F.col(\"name\").contains(\"hair dryer\"), \"hair dryer\")\n",
    "         .otherwise(F.col(\"name\"))\n",
    "    )\n",
    "\n",
    "    # 3. Determine Chosen Amenities (Local Top K + Global)\n",
    "    local_top = (amenities_long.groupBy(\"name\").count().orderBy(F.desc(\"count\")).limit(top_k_local))\n",
    "    local_list = [r[\"name\"] for r in local_top.collect()]\n",
    "    global_norm = [g.lower() for g in global_list]\n",
    "    chosen_amenities = list(set(local_list + global_norm))\n",
    "\n",
    "    # 4. Manual Pivot (Faster!) - Create 0/1 columns\n",
    "    aggs = []\n",
    "    amen_cols = []\n",
    "    \n",
    "    for amen in chosen_amenities:\n",
    "        safe_name = \"amen_\" + amen.replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "        amen_cols.append(safe_name)\n",
    "        # אם המילה מופיעה ברשימה של הנכס = 1, אחרת 0\n",
    "        aggs.append(F.max(F.when(F.col(\"name\") == amen, 1.0).otherwise(0.0)).alias(safe_name))\n",
    "    \n",
    "    # Group by PID and calculate all flags at once\n",
    "    features_df = amenities_long.groupBy(\"pid\").agg(*aggs)\n",
    "    \n",
    "    # Join back to main DF\n",
    "    df_out = df_in.join(features_df, df_in[id_col] == features_df[\"pid\"], \"left\")\n",
    "    \n",
    "    # Fill nulls with 0\n",
    "    df_out = df_out.fillna(0.0, subset=amen_cols)\n",
    "    \n",
    "    return df_out, amen_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dd412c1-87f1-4754-a9ed-2086e07cc6be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Load Data\n",
    "raw_df = spark.read.format(\"delta\").load(INPUT_PATH)\n",
    "\n",
    "# 2. Get Neighbors\n",
    "df_similar = get_neighbors_by_geo_bucket(raw_df, TARGET_PROPERTY_ID)\n",
    "\n",
    "# 3. Feature Extraction\n",
    "df_similar = extract_features(df_similar, FX_RATES_DF)\n",
    "\n",
    "# 4. Amenities Features\n",
    "df_final, amenity_cols = build_amenity_feature_df(\n",
    "    df_similar, \n",
    "    ID_COL, \n",
    "    top_k_local=20, \n",
    "    global_list=GLOBAL_IMPORTANT_AMENITIES\n",
    ")\n",
    "\n",
    "# 5. Define final list of columns for the model\n",
    "FINAL_NUMERIC_COLS = [c for c in (NUMERIC_FEATURES_BASE + CATEGORY_RATING_COLS) if c in df_final.columns]\n",
    "FINAL_BOOL_COLS = [c for c in BOOL_FEATURES_BASE if c in df_final.columns]\n",
    "FINAL_AMENITY_COLS = amenity_cols\n",
    "FINAL_CATEGORICAL_COLS = [c for c in CATEGORICAL_FEATURES if c in df_final.columns]\n",
    "\n",
    "ALL_FEATURES_RAW = FINAL_NUMERIC_COLS + FINAL_BOOL_COLS + FINAL_AMENITY_COLS\n",
    "\n",
    "print(f\"Total features: {len(ALL_FEATURES_RAW) + len(FINAL_CATEGORICAL_COLS)}\")\n",
    "# Cache to speed up training\n",
    "df_final.cache()\n",
    "print(\"Data ready and cached.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e109351-b208-447e-a152-804a99ed9db5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Stages setup\n",
    "stages = []\n",
    "\n",
    "# 1. Imputer (Median)\n",
    "imputer = Imputer(inputCols=ALL_FEATURES_RAW, outputCols=[f\"{c}__imp\" for c in ALL_FEATURES_RAW]).setStrategy(\"median\")\n",
    "stages.append(imputer)\n",
    "imp_cols = [f\"{c}__imp\" for c in ALL_FEATURES_RAW]\n",
    "\n",
    "# 2. OHE for Categoricals\n",
    "ohe_cols = []\n",
    "if FINAL_CATEGORICAL_COLS:\n",
    "    indexer = StringIndexer(inputCols=FINAL_CATEGORICAL_COLS, outputCols=[f\"{c}__idx\" for c in FINAL_CATEGORICAL_COLS], handleInvalid=\"keep\")\n",
    "    encoder = OneHotEncoder(inputCols=[f\"{c}__idx\" for c in FINAL_CATEGORICAL_COLS], outputCols=[f\"{c}__ohe\" for c in FINAL_CATEGORICAL_COLS], handleInvalid=\"keep\")\n",
    "    stages.extend([indexer, encoder])\n",
    "    ohe_cols = [f\"{c}__ohe\" for c in FINAL_CATEGORICAL_COLS]\n",
    "\n",
    "# 3. Assembler & Scaler\n",
    "assembler = VectorAssembler(inputCols=imp_cols + ohe_cols, outputCol=\"features_vec\", handleInvalid=\"keep\")\n",
    "scaler = StandardScaler(inputCol=\"features_vec\", outputCol=\"features_scaled\", withMean=True, withStd=True)\n",
    "\n",
    "# 4. Model (Linear Regression)\n",
    "lr = LinearRegression(featuresCol=\"features_scaled\", labelCol=LABEL_COL, maxIter=50, regParam=0.1, elasticNetParam=0.5)\n",
    "\n",
    "stages.extend([assembler, scaler, lr])\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "# Train\n",
    "train_df = df_final.filter((F.col(LABEL_COL).isNotNull()) & (F.col(ID_COL) != F.lit(TARGET_PROPERTY_ID)))\n",
    "model = pipeline.fit(train_df)\n",
    "\n",
    "print(\"Model trained successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "220d1879-7b33-49d7-b7eb-64174f5a81d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import base64\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "import regex as re\n",
    "\n",
    "# --- 1. Chart Generation Helper (Base64) ---\n",
    "def fig_to_base64(fig):\n",
    "    \"\"\" Helper: Convert matplotlib figure to Base64 string \"\"\"\n",
    "    buf = io.BytesIO()\n",
    "    fig.savefig(buf, format='png', bbox_inches='tight')\n",
    "    buf.seek(0)\n",
    "    img_str = base64.b64encode(buf.read()).decode('utf-8')\n",
    "    plt.close(fig) \n",
    "    return img_str\n",
    "\n",
    "def humanize_feature_name(s: str) -> str:\n",
    "    if s is None:\n",
    "        return s\n",
    "    s = str(s)\n",
    "\n",
    "    # remove leading 'amen_' (only if it's a prefix)\n",
    "    s = re.sub(r\"^amen_\", \"\", s)\n",
    "\n",
    "    # remove trailing '_imp' (only if it's a suffix)\n",
    "    s = re.sub(r\"_imp$\", \"\", s)\n",
    "\n",
    "    # underscores -> spaces\n",
    "    s = s.replace(\"_\", \" \")\n",
    "\n",
    "    # clean extra spaces\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "    # nicer casing\n",
    "    return s.title()\n",
    "\n",
    "\n",
    "def generate_charts_data(property_output_df, topN=10):\n",
    "    \"\"\"Generates charts and returns them as Base64 strings for the UI\"\"\"\n",
    "    pdf = property_output_df.toPandas().copy()\n",
    "    images_data = {}\n",
    "\n",
    "    # Create a human-readable label column for display\n",
    "    if \"feature\" in pdf.columns:\n",
    "        pdf[\"feature_label\"] = pdf[\"feature\"].apply(humanize_feature_name)\n",
    "    else:\n",
    "        pdf[\"feature_label\"] = \"Unknown\"\n",
    "\n",
    "    # Chart 1: Top absolute effects (impact %)\n",
    "    df1 = pdf.sort_values(\"impact_pct_abs\", ascending=False).head(topN).iloc[::-1]\n",
    "    if not df1.empty:\n",
    "        fig1 = plt.figure(figsize=(10, 5))\n",
    "        plt.barh(df1[\"feature_label\"], df1[\"impact_pct_abs\"], color=\"skyblue\")\n",
    "        plt.title(f\"Top {topN} Feature Effects on Predicted Score (%)\")\n",
    "        plt.xlabel(\"Absolute Impact (%)\")\n",
    "        plt.tight_layout()\n",
    "        images_data[\"impacts_chart\"] = fig_to_base64(fig1)\n",
    "\n",
    "    # Chart 2: Improvement opportunities (estimated gain)\n",
    "    df2 = (\n",
    "        pdf[pdf[\"potential_gain_scaled\"] > 0.01]\n",
    "        .sort_values(\"potential_gain_scaled\", ascending=False)\n",
    "        .head(topN)\n",
    "        .iloc[::-1]\n",
    "    )\n",
    "\n",
    "    if not df2.empty:\n",
    "        fig2 = plt.figure(figsize=(10, 5))\n",
    "        plt.barh(df2[\"feature_label\"], df2[\"potential_gain_scaled\"], color=\"lightgreen\")\n",
    "        plt.title(f\"Top Improvement Opportunities (Estimated Gain)\")\n",
    "        plt.xlabel(\"Estimated Gain (score points)\")\n",
    "        plt.tight_layout()\n",
    "        images_data[\"gain_chart\"] = fig_to_base64(fig2)\n",
    "    else:\n",
    "        images_data[\"gain_chart\"] = None\n",
    "\n",
    "    return images_data\n",
    "\n",
    "\n",
    "# --- 2. Main Logic: Explanation & JSON Builder ---\n",
    "def explain_property_output(df, train_df, model, property_id, id_col, label_col, features_raw, categorical_cols=None, topK=60):\n",
    "    \"\"\" Generates the explanation table optimized for AI Agents \"\"\"\n",
    "    if categorical_cols is None: categorical_cols = []\n",
    "\n",
    "    # --- A. Get Target & Top Group Data ---\n",
    "    target_row = df.filter(F.col(id_col) == F.lit(property_id)).limit(1)\n",
    "    if target_row.count() == 0: raise ValueError(f\"Property {property_id} not found\")\n",
    "    \n",
    "    # Find Top 20% Benchmark\n",
    "    q = train_df.approxQuantile(label_col, [0.8], 0.001)[0]\n",
    "    top_df = train_df.filter(F.col(label_col) >= q)\n",
    "    \n",
    "    # Calculate Averages for Numerics\n",
    "    mean_exprs = [F.avg(c).alias(c) for c in features_raw if c in top_df.columns]\n",
    "    top_mean_row = top_df.agg(*mean_exprs).withColumn(id_col, F.lit(\"TOP_MEAN\"))\n",
    "\n",
    "    # Add Mode for Categoricals (Prevent Nulls crashing the model)\n",
    "    for cat_col in categorical_cols:\n",
    "        if cat_col in top_df.columns:\n",
    "            mode_row = top_df.groupBy(cat_col).count().orderBy(F.desc(\"count\")).limit(1).collect()\n",
    "            mode_val = mode_row[0][cat_col] if mode_row else \"unknown\"\n",
    "            top_mean_row = top_mean_row.withColumn(cat_col, F.lit(mode_val))\n",
    "\n",
    "    # --- B. Run Model Transform ---\n",
    "    target_scored = model.transform(target_row)\n",
    "    top_scored = model.transform(top_mean_row)\n",
    "    \n",
    "    # --- C. Extract Metadata & Coefficients ---\n",
    "    attrs = target_scored.schema[\"features_vec\"].metadata[\"ml_attr\"][\"attrs\"]\n",
    "    feature_names_vec = []\n",
    "    all_attrs = []\n",
    "    if \"numeric\" in attrs: all_attrs.extend(attrs[\"numeric\"])\n",
    "    if \"binary\" in attrs: all_attrs.extend(attrs[\"binary\"])\n",
    "    if \"nominal\" in attrs: all_attrs.extend(attrs[\"nominal\"])\n",
    "    all_attrs.sort(key=lambda x: x[\"idx\"])\n",
    "    feature_names_vec = [a[\"name\"] for a in all_attrs]\n",
    "\n",
    "    lr_model = model.stages[-1]\n",
    "    coefs = lr_model.coefficients.toArray().tolist()\n",
    "    coef_df = spark.createDataFrame(list(zip(feature_names_vec, coefs)), [\"feature\", \"coef\"])\n",
    "    \n",
    "    # Extract Vectors\n",
    "    target_vec = target_scored.select(vector_to_array(\"features_scaled\").alias(\"vec\")).collect()[0][\"vec\"]\n",
    "    top_vec = top_scored.select(vector_to_array(\"features_scaled\").alias(\"vec\")).collect()[0][\"vec\"]\n",
    "    \n",
    "    vec_df = spark.createDataFrame(\n",
    "        [(i, float(target_vec[i]), float(top_vec[i])) for i in range(len(target_vec))],\n",
    "        [\"idx\", \"target_scaled\", \"top_scaled\"]\n",
    "    )\n",
    "    \n",
    "    idx_map = spark.createDataFrame(list(enumerate(feature_names_vec)), [\"idx\", \"feature\"])\n",
    "    \n",
    "    # --- D. Melt Raw Values (Helper) ---\n",
    "    def melt_raw(row_df, val_name):\n",
    "        stack_parts = []\n",
    "        valid_cols = [c for c in features_raw if c in row_df.columns]\n",
    "        if not valid_cols: return spark.createDataFrame([], f\"feature string, {val_name} double\")\n",
    "        for c in valid_cols:\n",
    "            stack_parts.append(f\"'{c}__imp', `{c}`\") \n",
    "        stack_expr = f\"stack({len(valid_cols)}, {', '.join(stack_parts)}) as (feature, {val_name})\"\n",
    "        return row_df.select(F.expr(stack_expr))\n",
    "\n",
    "    target_raw_long = melt_raw(target_row, \"target_value\")\n",
    "    top_raw_long = melt_raw(top_mean_row, \"top_mean_value\")\n",
    "\n",
    "    # --- E. Join All & Calculate Impacts ---\n",
    "    final_df = (\n",
    "        idx_map\n",
    "        .join(coef_df, \"feature\")\n",
    "        .join(vec_df, \"idx\")\n",
    "        .join(target_raw_long, \"feature\", \"left\")\n",
    "        .join(top_raw_long, \"feature\", \"left\")\n",
    "        .withColumn(\"impact_value\", F.col(\"coef\") * F.col(\"target_scaled\"))\n",
    "        .withColumn(\"potential_gain_scaled\", F.col(\"coef\") * (F.col(\"top_scaled\") - F.col(\"target_scaled\")))\n",
    "        .withColumn(\"abs_impact\", F.abs(F.col(\"impact_value\")))\n",
    "        .orderBy(F.desc(\"abs_impact\"))\n",
    "    )\n",
    "    \n",
    "    total_impact = final_df.agg(F.sum(\"abs_impact\")).collect()[0][0] or 1.0\n",
    "    final_df = final_df.withColumn(\"impact_pct_abs\", (F.col(\"abs_impact\") / total_impact) * 100)\n",
    "    \n",
    "    # --- F. PREPARE FOR AGENT (Cleanup & Filtering) ---\n",
    "    \n",
    "    # 1. Add verbal trend instead of raw coefficient number\n",
    "    df_clean = final_df.withColumn(\n",
    "        \"market_trend\", \n",
    "        F.when(F.col(\"coef\") > 0, \"Positive (Good to have)\")\n",
    "         .when(F.col(\"coef\") < 0, \"Negative (Avoid)\")\n",
    "         .otherwise(\"Neutral\")\n",
    "    )\n",
    "\n",
    "    # 3. Select & Rename for LLM Clarity (The Lean Payload)\n",
    "    df_clean = df_clean.select(\n",
    "        F.col(\"feature\").alias(\"name\"),\n",
    "        F.col(\"target_value\").alias(\"my_value\"),      \n",
    "        F.col(\"top_mean_value\").alias(\"market_avg\"),\n",
    "        F.col(\"market_trend\"),                         \n",
    "        F.col(\"impact_value\").alias(\"current_impact\"),\n",
    "        F.col(\"potential_gain_scaled\").alias(\"opportunity\"), \n",
    "        F.col(\"impact_pct_abs\").alias(\"importance_pct\") \n",
    "    )\n",
    "\n",
    "    # 4. Convert to JSON List\n",
    "    top_rows = df_clean.limit(topK).toJSON().collect()\n",
    "    json_list = [json.loads(r) for r in top_rows]\n",
    "    \n",
    "    # Return both the full DF (for charts) and the list (for LLM)\n",
    "    return final_df, json_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0ffad0b-4c53-48a8-995f-96cc89616276",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import base64\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# 1. Get Parameter (Default for testing)\n",
    "dbutils.widgets.text(\"property_id\", \"40458495\") \n",
    "target_property_id = dbutils.widgets.get(\"property_id\")\n",
    "\n",
    "print(f\"\uD83D\uDD75️ Analyzing Property: {target_property_id}...\")\n",
    "\n",
    "try:\n",
    "    # 2. Run Analysis (Laucnhed Logic)\n",
    "    df_full, insights_list = explain_property_output(\n",
    "        df=df_final,\n",
    "        train_df=train_df,\n",
    "        model=model,\n",
    "        property_id=target_property_id, \n",
    "        id_col=ID_COL,\n",
    "        label_col=LABEL_COL,\n",
    "        features_raw=ALL_FEATURES_RAW,\n",
    "        categorical_cols=FINAL_CATEGORICAL_COLS\n",
    "    )\n",
    "\n",
    "    # 3. Generate Charts (Base64 Strings)\n",
    "    charts_b64 = generate_charts_data(df_full, topN=10)\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"\uD83D\uDCCA VISUAL CHECK (For Developer)\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    if not charts_b64:\n",
    "        print(\"No charts generated.\")\n",
    "    \n",
    "    for chart_name, b64_str in charts_b64.items():\n",
    "        if b64_str:\n",
    "            print(f\"\\nDisplaying: {chart_name}\")\n",
    "            display(Image(data=base64.b64decode(b64_str)))\n",
    "        else:\n",
    "            print(f\"\\n{chart_name}: No data (likely empty)\")\n",
    "    \n",
    "    print(\"=\"*40 + \"\\n\")\n",
    "    # ---------------------------------------------\n",
    "\n",
    "    # 4. Construct Final Response Payload\n",
    "    final_output = {\n",
    "        \"status\": \"success\",\n",
    "        \"property_id\": target_property_id,\n",
    "        \"llm_context\": {\n",
    "            \"insights\": insights_list,\n",
    "            \"summary_note\": \"market_trend indicates sentiment. opportunity is potential gain.\"\n",
    "        },\n",
    "        \"ui_artifacts\": {\n",
    "            \"charts\": charts_b64\n",
    "        }\n",
    "    }\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error occurred: {e}\")\n",
    "    final_output = {\n",
    "        \"status\": \"error\",\n",
    "        \"message\": str(e)\n",
    "    }\n",
    "\n",
    "# 5. Return to Agent\n",
    "output_json = json.dumps(final_output, ensure_ascii=False)\n",
    "print(\"✅ Output Payload Ready.\")\n",
    "dbutils.notebook.exit(output_json)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Linear Regression Model",
   "widgets": {
    "property id": {
     "currentValue": "40458495",
     "nuid": "09f49bd1-6724-4e98-812c-10c1ab0c405d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "40458495",
      "label": null,
      "name": "property id",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "40458495",
      "label": null,
      "name": "property id",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}